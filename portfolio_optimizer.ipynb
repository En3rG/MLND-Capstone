{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gio/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Gio/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, make_scorer, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import pickle\n",
    "from numpy import array as np_array\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from IPython.display import display                                 # Allows the use of display() for DataFrames\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start time.  Used to measure execution time.\n",
    "START = time.clock()\n",
    "\n",
    "DATA_SOURCE = 'google'\n",
    "\n",
    "## Date range used for training data\n",
    "TRAINING_START_DATE = '2010-01-01'\n",
    "TRAINING_END_DATE = '2017-7-31'\n",
    "\n",
    "## Date range used for testing data\n",
    "TEST_START_DATE = '2016-06-01'\n",
    "TEST_END_DATE = '2017-7-31'\n",
    "\n",
    "## Stock tickers for training data\n",
    "TRAINING_TICKERS = ['AAPL', 'GOOG', 'YHOO', 'T', 'IMAX', 'IBM', 'NFLX', 'SIRI', 'S', 'PLUG', \\\n",
    "           'C', 'BAC', 'P', 'NOK', 'XONE', 'SSYS', 'TSLA', 'AMZN', 'SDRL', 'DDD', \\\n",
    "           'DBO', 'SRPT', 'SPWR', 'SCTY', 'FB', 'URRE', 'NQ', 'TWTR', 'F', 'BAH', \\\n",
    "           'MZDAY', 'FSYS', 'BIDU', 'KORS', 'HLF', 'ORCL', 'MBLY']\n",
    "#TRAINING_TICKERS = ['AAPL', 'GOOG', 'YHOO','SSYS','SPWR','SDRL']\n",
    "#TRAINING_TICKERS = ['AAPL']\n",
    "\n",
    "## Stock tickers for testing data\n",
    "#TESTING_TICKERS = ['SHOP', 'BA', 'SD', 'FCEL', 'HEMP', 'TPLM', 'CHK', 'OLED', 'HON', 'LMT', 'CMG', 'MA']\n",
    "TESTING_TICKERS = ['BA','OLED', 'HON','MA','TPLM', 'SD', 'FCEL', 'CHK', 'CMG']  ## unable to read SHOP,HEMP,LMT\n",
    "#TESTING_TICKERS = ['SD', 'FCEL', 'CHK', 'CMG']\n",
    "#TESTING_TICKERS = ['AAPL']\n",
    "#TESTING_TICKERS = ['BA','OLED', 'HON','MA']\n",
    "#TESTING_TICKERS = ['SD', 'FCEL', 'CHK', 'CMG']\n",
    "\n",
    "## Initial money to be invested\n",
    "MONEY = 10000\n",
    "\n",
    "## Commision rate when buying/selling stocks\n",
    "COMM_RATE = 4.95\n",
    "\n",
    "## Long term capital gain tax rate (percentage)\n",
    "GAIN_LONG = 0.15\n",
    "\n",
    "## Short term capital gain tax rate (percentage).  Also used for losses, assuming its the individuals tax bracket\n",
    "GAIN_SHORT = 0.25\n",
    "\n",
    "## Models predict the ratio base on these targets\n",
    "TARGET_RATIOS = ['vClose_pc','vr2_pc','vr3_pc','vr5_pc','vr10_pc','vr15_pc','vr25_pc','vr40_pc']\n",
    "#TARGET_RATIOS = ['vr40_pc']\n",
    "## New target ratio, will not just be relative to close\n",
    "#TARGET_RATIOS = ['Close_pc','r2_p2','r3_p3','r5_p5','r10_p10','r15_p15','r25_p25','r40_p40']\n",
    "#TARGET_RATIOS = ['r40_p40']\n",
    "\n",
    "## Used to determine when predictions will be a buy/sell\n",
    "#SELL_BUY_VALUES = [(1,1)]\n",
    "#SELL_BUY_VALUES = [(0.99,0.99), (1,1),(0.98,1.02),(0.99,1.01),(1.00,1.02),(1.00,1.03)]\n",
    "SELL_BUY_VALUES = [(0.985,0.985),(0.99,0.99),(0.995,0.995), (1,1),(1.005,1.005),(1.01,1.01),(1.015,1.015), \\\n",
    "                   (0.985,1.015),(0.99,1.01),(0.995,1.005),\\\n",
    "                   (0.995,1.00),(1.00,1.005),(1.005,1.01), \\\n",
    "                   (1.00,1.015),(1.00,1.01)]\n",
    "\n",
    "## Set to True if we are reading existing models\n",
    "## Set to False to generate new models\n",
    "READ_EXISTING_MODEL = False\n",
    "\n",
    "## Set True if using multiprocessing\n",
    "MULTIPROCESSOR = True\n",
    "\n",
    "## Number of processes for multiprocessing pool\n",
    "NUM_PROCESSES = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the stocks data.  Save data as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_training_data():\n",
    "    all_weekdays = pd.date_range(start=TRAINING_START_DATE,end=TRAINING_END_DATE,freq='B')\n",
    "    panel_data = data.DataReader(TRAINING_TICKERS,DATA_SOURCE,TRAINING_START_DATE,TRAINING_END_DATE) ## panel type\n",
    "    panel_data.to_pickle('training_stocks.pkl')                                                      ## save to pickle\n",
    "\n",
    "def gather_testing_data():\n",
    "    all_weekdays = pd.date_range(start=TEST_START_DATE,end=TEST_END_DATE,freq='B')\n",
    "    panel_data = data.DataReader(TESTING_TICKERS,DATA_SOURCE,TEST_START_DATE,TEST_END_DATE)           \n",
    "    panel_data.to_pickle('testing_stocks.pkl')  \n",
    "    \n",
    "gather_training_data()                                           ## can comment out if pickle file is already locally\n",
    "gather_testing_data()                                            ## can comment out if pickle file is already locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get the rolling averages. 2, 3, 5, 10, 15, 25 and 40-day moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rolling(df):\n",
    "    stock = df['Close']\n",
    "    r2 = stock.rolling(window=2).mean()\n",
    "    r3 = stock.rolling(window=3).mean()\n",
    "    r5 = stock.rolling(window=5).mean()\n",
    "    r10 = stock.rolling(window=10).mean()\n",
    "    r15 = stock.rolling(window=15).mean()\n",
    "    r25 = stock.rolling(window=25).mean()\n",
    "    r40 = stock.rolling(window=40).mean()\n",
    "\n",
    "    return r2, r3, r5, r10, r15, r25, r40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stock(tick, df):\n",
    "    close = df['vClose_pc']\n",
    "    r2 = df['vr2_pc']\n",
    "    r3 = df['vr3_pc']\n",
    "    r5 = df['vr5_pc']\n",
    "    r10 = df['vr10_pc']\n",
    "    r15 = df['vr15_pc']\n",
    "    r25 = df['vr25_pc']\n",
    "    r40 = df['vr40_pc']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(close.index,close,label=tick)\n",
    "    ax.plot(r2.index, r2, label='2 days rolling')\n",
    "    ax.plot(r3.index, r3, label='3 days rolling')\n",
    "    ax.plot(r5.index, r5, label='5 days rolling')\n",
    "    ax.plot(r10.index, r10, label='10 days rolling')\n",
    "    ax.plot(r15.index, r15, label='15 days rolling')\n",
    "    ax.plot(r25.index, r25, label='25 days rolling')\n",
    "    ax.plot(r40.index, r40, label='40 days rolling')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Closing prices ($)')\n",
    "    ax.legend()\n",
    "\n",
    "    #plt.show()\n",
    "    fig.savefig(\"figures/\"+tick+'.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a DataFrame of the entire list of stocks for training, from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_stocks_df(filename):\n",
    "    main_df = pd.DataFrame()\n",
    "    dfs = []\n",
    "    panel_data = pd.read_pickle(filename)                                           ## read saved stocks data\n",
    "\n",
    "    for tick in TRAINING_TICKERS:\n",
    "        ## Extract single stock from panel_data\n",
    "        df = panel_data[:,:,tick]                                                   ## becomes df type, from panel\n",
    "        df = get_stock_df(df,tick)\n",
    "        \n",
    "        ## Plot stock\n",
    "        plot_stock(tick,df)\n",
    "        \n",
    "        #main_df = main_df.append(df)                                     \n",
    "        dfs.append(df)                                           ## faster to append once, with [] of df\n",
    "\n",
    "    main_df = main_df.append(dfs)                                ## faster to append once, with [] of df\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get DataFrame of individual stocks.  Generates 106 columns, 17 columns will be removed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List columns generated here:\n",
    "\n",
    "\"\"\"\n",
    "def get_stock_df(df,tick):\n",
    "    r2, r3, r5, r10, r15, r25, r40 = get_rolling(df)                            ## get moving averages\n",
    "\n",
    "    ## Rename Volume column then remove it\n",
    "    vol = df['Volume']\n",
    "    vol = vol.to_frame()\n",
    "    vol.columns = ['Vol']\n",
    "    df = df.drop('Volume', 1)                                                   ## 1 for axis 1, which is column\n",
    "\n",
    "    ## Rename columns\n",
    "    df_r2= r2.to_frame()                                                        ## from series to df\n",
    "    df_r2.columns = ['r2']                                                      ## change column title\n",
    "\n",
    "    df_r3= r3.to_frame()\n",
    "    df_r3.columns = ['r3']\n",
    "\n",
    "    df_r5= r5.to_frame()\n",
    "    df_r5.columns = ['r5']\n",
    "\n",
    "    df_r10= r10.to_frame()                                                      \n",
    "    df_r10.columns = ['r10']                                                    \n",
    "\n",
    "    df_r15= r15.to_frame()\n",
    "    df_r15.columns = ['r15']\n",
    "\n",
    "    df_r25= r25.to_frame()\n",
    "    df_r25.columns = ['r25']\n",
    "\n",
    "    df_r40= r40.to_frame()\n",
    "    df_r40.columns = ['r40']\n",
    "\n",
    "    ## Shift rows to generate next/previous values\n",
    "    #predict = df['Close'].copy()\n",
    "    predict = df['Close'].shift(-1)                                          ## a series\n",
    "    predict = predict.to_frame()\n",
    "    predict.columns = ['predict']\n",
    "\n",
    "    prev_close = df['Close'].shift(1)\n",
    "    prev_close = prev_close.to_frame()\n",
    "    prev_close.columns = ['prev_close']\n",
    "\n",
    "    prev_r2 = df_r2['r2'].shift(1)\n",
    "    prev_r2 = prev_r2.to_frame()\n",
    "    prev_r2.columns = ['prev_r2']\n",
    "\n",
    "    prev_r3 = df_r3['r3'].shift(1)\n",
    "    prev_r3 = prev_r3.to_frame()\n",
    "    prev_r3.columns = ['prev_r3']\n",
    "\n",
    "    prev_r5 = df_r5['r5'].shift(1)\n",
    "    prev_r5 = prev_r5.to_frame()\n",
    "    prev_r5.columns = ['prev_r5']\n",
    "\n",
    "    prev_r10 = df_r10['r10'].shift(1)\n",
    "    prev_r10 = prev_r10.to_frame()\n",
    "    prev_r10.columns = ['prev_r10']\n",
    "\n",
    "    prev_r15 = df_r15['r15'].shift(1)\n",
    "    prev_r15 = prev_r15.to_frame()\n",
    "    prev_r15.columns = ['prev_r15']\n",
    "\n",
    "    prev_r25 = df_r25['r25'].shift(1)\n",
    "    prev_r25 = prev_r25.to_frame()\n",
    "    prev_r25.columns = ['prev_r25']\n",
    "\n",
    "    prev_r40 = df_r40['r40'].shift(1)\n",
    "    prev_r40 = prev_r40.to_frame()\n",
    "    prev_r40.columns = ['prev_r40']\n",
    "\n",
    "    ## Generate entire dataframe\n",
    "    ## encapsulate in a list for multiple df\n",
    "    df1 = predict.join([prev_close,prev_r2,prev_r3,prev_r5,prev_r10,prev_r15,prev_r25,prev_r40,vol]) \n",
    "    df2 = df.join([df_r2,df_r3,df_r5,df_r10,df_r15, df_r25, df_r40])\n",
    "    df3 = df2.copy()\n",
    "    df4 = df2.copy()\n",
    "    df5 = df2.copy()\n",
    "    df6 = df2.copy()\n",
    "    df6 = df2.copy()\n",
    "    df7 = df2.copy()\n",
    "    df8 = df2.copy()\n",
    "    df9 = df2.copy()\n",
    "    df10 = df2.copy()\n",
    "    ## will have original value (not percentage)\n",
    "    df10.columns = ['vOpen_pc','vHigh_pc','vLow_pc','vClose_pc','vr2_pc','vr3_pc','vr5_pc','vr10_pc','vr15_pc','vr25_pc','vr40_pc']\n",
    "    ## will be with respect to prev_close\n",
    "    df2.columns = ['Open_pc','High_pc','Low_pc','Close_pc','r2_pc','r3_pc','r5_pc','r10_pc','r15_pc','r25_pc','r40_pc'] \n",
    "    ## will be with respect to prev_r2\n",
    "    df3.columns = ['Open_p2','High_p2','Low_p2','Close_p2','r2_p2','r3_p2','r5_p2','r10_p2','r15_p2','r25_p2','r40_p2']     \n",
    "    ## will be with respect to prev_r3\n",
    "    df4.columns = ['Open_p3','High_p3','Low_p3','Close_p3','r2_p3','r3_p3','r5_p3','r10_p3','r15_p3','r25_p3','r40_p3']     \n",
    "    ## will be with respect to prev_r5\n",
    "    df5.columns = ['Open_p5','High_p5','Low_p5','Close_p5','r2_p5','r3_p5','r5_p5','r10_p5','r15_p5','r25_p5','r40_p5']     \n",
    "    ## will be with respect to prev_r10\n",
    "    df6.columns = ['Open_p10','High_p10','Low_p10','Close_p10','r2_p10','r3_p10','r5_p10','r10_p10','r15_p10','r25_p10','r40_p10']     \n",
    "    ## will be with respect to prev_r15\n",
    "    df7.columns = ['Open_p15','High_p15','Low_p15','Close_p15','r2_p15','r3_p15','r5_p15','r10_p15','r15_p15','r25_p15','r40_p15']     \n",
    "    ## will be with respect to prev_r25\n",
    "    df8.columns = ['Open_p25','High_p25','Low_p25','Close_p25','r2_p25','r3_p25','r5_p25','r10_p25','r15_p25','r25_p25','r40_p25']     \n",
    "    ## will be with respect to prev_r40\n",
    "    df9.columns = ['Open_p40','High_p40','Low_p40','Close_p40','r2_p40','r3_p40','r5_p40','r10_p40','r15_p40','r25_p40','r40_p40']     \n",
    "\n",
    "    ## Combine all to one dataframe\n",
    "    df = df1.join([df10,df2,df3,df4,df5,df6,df7,df8,df9])\n",
    "\n",
    "    ## Drop N/A\n",
    "    df = df.dropna(axis=0,how='any')                                            ## drop rows containing at least 1 NA\n",
    "\n",
    "    ## Normalize columns, base on previous values.  Get ratios/percentage \n",
    "    #df[columns_to_divide] = df[columns_to_divide] / df['prev_close']           ## having size issues\n",
    "\n",
    "    cols_to_divide = ['Open_pc','High_pc','Low_pc','Close_pc','r2_pc','r3_pc','r5_pc','r10_pc','r15_pc','r25_pc','r40_pc']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_close'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p2','High_p2','Low_p2','Close_p2','r2_p2','r3_p2','r5_p2','r10_p2','r15_p2','r25_p2','r40_p2']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r2'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p3','High_p3','Low_p3','Close_p3','r2_p3','r3_p3','r5_p3','r10_p3','r15_p3','r25_p3','r40_p3']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r3'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p5','High_p5','Low_p5','Close_p5','r2_p5','r3_p5','r5_p5','r10_p5','r15_p5','r25_p5','r40_p5']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r5'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p10','High_p10','Low_p10','Close_p10','r2_p10','r3_p10','r5_p10','r10_p10','r15_p10','r25_p10','r40_p10']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r10'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p15','High_p15','Low_p15','Close_p15','r2_p15','r3_p15','r5_p15','r10_p15','r15_p15','r25_p15','r40_p15']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r15'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p25','High_p25','Low_p25','Close_p25','r2_p25','r3_p25','r5_p25','r10_p25','r15_p25','r25_p25','r40_p25']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r25'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p40','High_p40','Low_p40','Close_p40','r2_p40','r3_p40','r5_p40','r10_p40','r15_p40','r25_p40','r40_p40']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r40'].values,axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update target, base on target ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(X,target_ratio):\n",
    "    ## update predict base on target_ratio\n",
    "    ## All these are base on predict (relative to close only)\n",
    "    cols_to_divide = ['predict']\n",
    "    X[cols_to_divide] = X[cols_to_divide].div(X[target_ratio].values,axis=0)\n",
    "    \n",
    "    ## New targets prev target_ratio/current target ratio\n",
    "    ## Shift rows to generate next values\n",
    "#     target = X[target_ratio].shift(-1) \n",
    "#     #target = X[target_ratio].copy()\n",
    "#     X['predict'] = target\n",
    "#     ## Drop N/A\n",
    "#     X = X.dropna(axis=0,how='any')  \n",
    "    \n",
    "    ## Convert target base on sell/buy prices thats provided\n",
    "    y = convert_target_value(X['predict'],sell_below,buy_above)\n",
    "    \n",
    "    ## Delete unneccesary columns\n",
    "    del X['predict'],X['prev_close'],X['prev_r2'],X['prev_r3'],X['prev_r5']\n",
    "    del X['prev_r10'],X['prev_r15'],X['prev_r25'],X['prev_r40']\n",
    "    ## Delete columns with actual price\n",
    "    del X['vOpen_pc'],X['vHigh_pc'],X['vLow_pc'],X['vClose_pc'],X['vr2_pc']\n",
    "    del X['vr3_pc'],X['vr5_pc'],X['vr10_pc'],X['vr15_pc'],X['vr25_pc'],X['vr40_pc']\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to convert target.  Buy (1), Sell (-1), or Neutral (0).  Base on sell/buy prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_target_value(arr,sell_below,buy_above):\n",
    "    ans = []\n",
    "\n",
    "    for x in arr:\n",
    "        if x <= sell_below:\n",
    "            ans.append(-1)\n",
    "        elif x >= buy_above:\n",
    "            ans.append(1)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model():\n",
    "    ## create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300,input_dim=89,activation='tanh',kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(150,activation='tanh'))\n",
    "    model.add(Dense(3,activation='softmax'))   \n",
    "    \n",
    "    #model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])        ## for binary classifier\n",
    "    #model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adamax',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='nadam',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='tfoptimizer',metrics=['accuracy'])\n",
    "    \n",
    "    opt = optimizers.SGD(lr=0.001,momentum=0.9,decay=1e-6,nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get encoding, for target in Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a (XXX,3) from (XXX,1), for the input of the neural network\n",
    "Didnt use np_utils.to_categorical() since output could be -1,1 or -1,0,1 depending on sell/buy values\n",
    "\"\"\"\n",
    "def myEncoder(arr):\n",
    "    s = (len(arr),3)\n",
    "    numpy_arr = np.zeros(s)\n",
    "    \n",
    "    for i,num in enumerate(arr):\n",
    "        if num == -1:\n",
    "            numpy_arr[i] = np_array([1,0,0])\n",
    "        elif num == 1:\n",
    "            numpy_arr[i] = np_array([0,0,1])\n",
    "        else:\n",
    "            numpy_arr[i] = np_array([0,1,0])\n",
    "\n",
    "    return numpy_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to decode, as 1 dimensional target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a (XXX,1) from (XXX,3), from predict of the neural network\n",
    "\"\"\"\n",
    "def myDecoder(numpy_arr):\n",
    "    arr = []\n",
    "    ## Models predictions are ndarray, percentages per category \n",
    "    for i,a in enumerate(numpy_arr):     \n",
    "        if np.argmax(a) == 0:\n",
    "            arr.append(-1)\n",
    "        elif np.argmax(a) == 2:\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "             \n",
    "    ## Convert the arr list to an ndarray.  \n",
    "    return np_array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpter function to normalize input data for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale_input(arr):\n",
    "    \"\"\"\n",
    "    Scales data to be between a - b\n",
    "    \"\"\"\n",
    "    ## Function below is not used.  Initially thought arr was a df\n",
    "    def scaler(x):\n",
    "        new_x = (((highest_scale-lowest_scale)*(x-min_input))/(max_input-min_input))+lowest_scale\n",
    "        return new_x\n",
    "        \n",
    "    ## Tried -1 to 1 before\n",
    "    lowest_scale = -1\n",
    "    highest_scale = 1\n",
    "    max_input = 1.15\n",
    "    min_input = 0.85\n",
    "    \n",
    "    #result = arr.applymap(scaler)    ## if its a df, use applymap\n",
    "    \n",
    "    ## arr is an ndarray.  Use vectorize instead of applymap\n",
    "    scaler = lambda x: (((highest_scale-lowest_scale)*(x-min_input))/(max_input-min_input))+lowest_scale\n",
    "    func = np.vectorize(scaler)                                      ## vectorize scaler function\n",
    "    result = func(arr)                                               ## pass arr to vectorized function                           \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the metrics of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_metrics(target_ratio,sell_below,buy_above,X):\n",
    "    temp_result = {}\n",
    "    temp_result['target_ratio'] = target_ratio\n",
    "    temp_result['sell_below'] = sell_below\n",
    "    temp_result['buy_above'] = buy_above\n",
    "    \n",
    "    X,y = get_target(X,target_ratio)\n",
    "    print(\"Input shape: {} Target shape: {}\".format(X.shape,len(y)))\n",
    "    print(\"Target found are: {}\".format(set(y)))\n",
    "    \n",
    "    ## Perform cross validation.  95% to have as much training data as possible.  \n",
    "    ## Also, performance will be base on totally different set of stocks\n",
    "    try: X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,train_size=0.95,stratify=y)\n",
    "    ## Getting error sometimes: The least populated class in y has only 1 member, which is too few. \n",
    "    ## The minimum number of labels for any class cannot be less than 2.\n",
    "    except: X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,train_size=0.95)\n",
    "        \n",
    "    print(\"Sample of training data:\")\n",
    "    print(\"Number of rows: {}. Number of columns: {}.\".format(len(X_train),len(X_train.columns)))\n",
    "    print(X_train.head())\n",
    "    \n",
    "    beta = 0.5\n",
    "    \n",
    "    ## Initialize Models\n",
    "    ## entropy for exploratory analysis, gini (default) to minimize misclassification, max_features default None\n",
    "    #clf1 = DecisionTreeClassifier(criterion=\"entropy\",random_state=0,max_features=None)\n",
    "    clf1 = DecisionTreeClassifier()\n",
    "    clf2 = GaussianNB()\n",
    "    ## kernel 'rbf' default, others are linear, poly, sigmoid, C is penalty parameter, default is 1\n",
    "    #clf3 = SVC(random_state=0,C=1)                                   ## <-- makes execution time 20x longer\n",
    "    ## defaults are 1 for learning rate and 50 for n_estimators\n",
    "    #clf4 = AdaBoostClassifier(random_state=0,learning_rate=0.7,n_estimators=50)  ##.7,50 91-99\n",
    "    clf4 = AdaBoostClassifier()\n",
    "    clf5 = neural_network_model()\n",
    "    \n",
    "    if READ_EXISTING_MODEL == True:\n",
    "        ## Read existing models\n",
    "        \n",
    "        ## For DecisionTree\n",
    "        with open(\"models/DecisionTree_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf1 = pickle.load(f)\n",
    "            \n",
    "        ## For GaussianNB\n",
    "        with open(\"models/GaussianNB_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf2 = pickle.load(f)\n",
    "    \n",
    "        ## For SVC model\n",
    "#         with open(\"models/SVC_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "#             clf3 = pickle.load(f)\n",
    "            \n",
    "        ## For Adaboost\n",
    "        with open(\"models/Adaboost_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf4 = pickle.load(f)\n",
    "          \n",
    "        ## For Neural Network\n",
    "        ## Load json and create model\n",
    "        json_file = open(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".json\",\"r\")\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        clf5 = model_from_json(loaded_model_json)\n",
    "        ## Load weights into new model\n",
    "        clf5.load_weights(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".h5\")\n",
    "        ## Compile, make sure its the same as above\n",
    "        opt = optimizers.SGD(lr=0.001,momentum=0.9,decay=1e-6,nesterov=True)\n",
    "        clf5.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    \n",
    "    else:\n",
    "        ## Generate new models\n",
    "        \n",
    "        ## Fit Data to DecisionTree Model\n",
    "        clf1.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/DecisionTree_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf1, f)\n",
    "\n",
    "        # Fit Data to GaussianNB Model\n",
    "        clf2.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/GaussianNB_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf2, f)\n",
    "        \n",
    "        # Fit Data to SVC Model\n",
    "#         clf3.fit(X_train,y_train)\n",
    "#         ## Save model to a file\n",
    "#         with open(\"models/SVC_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "#             pickle.dump(clf3, f)\n",
    "\n",
    "        # Fit Data to Adaboost Model\n",
    "        clf4.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/Adaboost_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf4, f)\n",
    "    \n",
    "        ## Fit Data to Neural Model\n",
    "        input_train = X_train.as_matrix(columns=None)                     ## convert df to numpy array\n",
    "        #np.savetxt(\"test1.csv\",input_train,delimiter=\",\")\n",
    "        input_train = rescale_input(input_train)\n",
    "        #print(\"Input_train:\",input_train[np.r_[0:5]])                    ## print first 5\n",
    "        \n",
    "        ### encode class values as integers \n",
    "        #encoder = LabelEncoder()\n",
    "        #encoder.fit(y_train)\n",
    "        #encoded_y = encoder.transform(y_train)\n",
    "        ### convert integers to dummy variables (i.e one hot encoded)\n",
    "        #dummy_y = np_utils.to_categorical(encoded_y)\n",
    "        #print(dummy_y)\n",
    "        dummy_y = myEncoder(y_train)\n",
    "        clf5.fit(input_train,dummy_y,epochs=20,batch_size=100)  \n",
    "        ## Serialize model to JSON\n",
    "        model_json = clf5.to_json()\n",
    "        with open(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".json\",\"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        ## Serialize weights to HDF5\n",
    "        clf5.save_weights(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".h5\")\n",
    "    \n",
    "    ## Get accuracy and fscore of the models \n",
    "    temp_result['DecisionTree Accuracy'] = accuracy_score(y_test, clf1.predict(X_test))\n",
    "    temp_result['DecisionTree Fscore'] = fbeta_score(y_test, clf1.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    temp_result['GaussianNB Accuracy'] = accuracy_score(y_test,clf2.predict(X_test))\n",
    "    temp_result['GaussianNB Fscore'] = fbeta_score(y_test, clf2.predict(X_test),beta,average='weighted')\n",
    "\n",
    "#     temp_result['SVC Accuracy'] = accuracy_score(y_test,clf3.predict(X_test))\n",
    "#     temp_result['SVC Fscore'] = fbeta_score(y_test, clf3.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    temp_result['Adaboost Accuracy'] = accuracy_score(y_test,clf4.predict(X_test))\n",
    "    temp_result['Adaboost Fscore'] = fbeta_score(y_test, clf4.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    input_test = X_test.as_matrix(columns=None)\n",
    "    input_test = rescale_input(input_test)\n",
    "    dummy_y = myEncoder(y_test)\n",
    "    #scores = clf5.evaluate(input_test, dummy_y)\n",
    "    #temp_result['NN Accuracy'] = scores[1]*100\n",
    "    temp_result['NN Accuracy'] = accuracy_score(y_test,myDecoder(clf5.predict(input_test)))\n",
    "    temp_result['NN Fscore'] = fbeta_score(y_test, myDecoder(clf5.predict(input_test)),beta,average='weighted')\n",
    "\n",
    "    ## Determine performance of the portfolio on each model\n",
    "    #models = [('DecisionTree',clf1),('GaussianNB',clf2),('Adaboost',clf4),('SVC',clf3),('NN',clf5)]\n",
    "    ## No SVC model\n",
    "    models = [('DecisionTree',clf1),('GaussianNB',clf2),('Adaboost',clf4),('NN',clf5)]\n",
    "    test_model_performance(models,temp_result,target_ratio,sell_below,buy_above)\n",
    "    \n",
    "    ## Check if multiprocessor is set\n",
    "    if MULTIPROCESSOR == False:\n",
    "        ## Result will contain all the result from each combination of the models\n",
    "        result.append(temp_result)   \n",
    "    else:\n",
    "        ## To support multiprocessing, will have to save for later\n",
    "        #temp_result.to_csv(\"temp/\"+str(time.clock())+\".csv\")      <-- temp_result is a dict, not df\n",
    "        with open(\"temp/\"+str(time.clock())+\".json\", 'w') as fp:\n",
    "            json.dump(temp_result, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_performance(models,temp_result,target_ratio,sell_below,buy_above):\n",
    "    panel_data = pd.read_pickle('testing_stocks.pkl')                   ## read saved stocks data\n",
    "\n",
    "    ## Initialize total to be 0 across all models\n",
    "    total = {'benchmark':0}\n",
    "    for model_name,_ in models:\n",
    "        total[model_name] = 0                                           ## will hold total money for that model\n",
    "        total[model_name+\"_transactions\"] = 0                           ## will hold total transactions for that model\n",
    "    \n",
    "    ## Go through each stock\n",
    "    for tick in TESTING_TICKERS:\n",
    "        ## Extract single stock from panel_data\n",
    "        #df = panel_data[:,:,tick]                                                   \n",
    "        #X = get_stock_df(df,tick)\n",
    "        ## We will now just read the test stocks, generated before\n",
    "        X = pd.DataFrame.from_csv(tick+'.csv')\n",
    "        \n",
    "        ## Delete unnecessary columns\n",
    "        del X['predict'],X['prev_close'],X['prev_r2'],X['prev_r3'],X['prev_r5']\n",
    "        del X['prev_r10'],X['prev_r15'],X['prev_r25'],X['prev_r40']\n",
    "        ## Delete columns with actual price\n",
    "        del X['vOpen_pc'],X['vHigh_pc'],X['vLow_pc'],X['vClose_pc'],X['vr2_pc']\n",
    "        del X['vr3_pc'],X['vr5_pc'],X['vr10_pc'],X['vr15_pc'],X['vr25_pc'],X['vr40_pc']\n",
    "        \n",
    "        ## Get predictions base on each models\n",
    "        for model_name, model in models:\n",
    "            ## predictions will be an ndarray\n",
    "            if model_name == 'NN':\n",
    "                input_test = X.as_matrix(columns=None)\n",
    "                input_test = rescale_input(input_test)\n",
    "                predictions = model.predict(input_test)\n",
    "                predictions = myDecoder(predictions)\n",
    "            else:\n",
    "                predictions = model.predict(X) \n",
    "                        \n",
    "            ## Add predictions to the dataframe\n",
    "            pred = pd.DataFrame(predictions.flatten(),index=X.index,columns=['Predictions'])\n",
    "            S = X.join(pred)\n",
    "            S['Transactions'] = 0                                 ## will contain number of transactions\n",
    "            S['Money'] = 0                                        ## will contain total current money amount\n",
    "            \n",
    "            ## Calculate transactions (NOT COMPLETE)\n",
    "            #temp_df = pd.DataFrame(X['Close_pc'].values,columns=['Close_pc'])\n",
    "            #temp_df = temp_df.join(pd.DataFrame(predictions.flatten(),columns=['Predictions']))\n",
    "            #temp_df['playing'] = temp_df['Predictions'].shift().eq(1)\n",
    "            ### cumprod of 'Close_pc' where 'playing' is True.  Then multiple with initial money\n",
    "            #temp_df['Money'] = temp_df['Close_pc'].where(temp_df['playing'],1).cumprod().mul(MONEY)\n",
    "            ### get just last value from 'Money'\n",
    "            #temp_result[tick+'_'+model_name] = float(format(temp_df['Money'].iloc[-1], '.2f'))           \n",
    "            #total[model_name] += float(format(temp_df['Money'].iloc[-1], '.2f'))\n",
    "            \n",
    "            ## Calculate transactions. SLOWER? BUT COMPLETE\n",
    "            playing = False                                        ## used to determine if currently in the market\n",
    "            money = copy.copy(MONEY)\n",
    "            transactions = 0\n",
    "            i = 0\n",
    "            for index, row in S.iterrows():\n",
    "                ## Update money\n",
    "                if i > 0 and playing == True:\n",
    "                    money = float(format(money*row['Close_pc'],'.2f'))\n",
    "                \n",
    "                ## Buy/Sell\n",
    "                if row['Predictions'] == 1:\n",
    "                    if playing == False:\n",
    "                        playing = True                            ## Buy, playing after this\n",
    "                        transactions += 1                         ## increment transaction number for this model\n",
    "                elif row['Predictions'] == -1:\n",
    "                    if playing == True:\n",
    "                        playing = False                           ## Sell, not playing after this\n",
    "                        transactions += 1                         ## increment transaction number for this model\n",
    "                i += 1        \n",
    "                \n",
    "                ## Change value in sample data S in index and column provided, with value/data provided\n",
    "                S.set_value(index,'Money',money)\n",
    "                S.set_value(index,'Transactions',transactions)\n",
    "                \n",
    "            ## Save dataframe for testing purposes\n",
    "            S.to_csv(\"data/\"+tick+\"_\"+model_name+\"_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".csv\")\n",
    "                    \n",
    "            ## If still playing at the end, we'll sell, thus increment number of transactions\n",
    "            transactions = transactions + 1 if playing == True else transactions\n",
    "            ## Will contain total money for this stock and model\n",
    "            temp_result[tick+'_'+model_name] = money\n",
    "            ## Will contain total number of transactions for this stock and model\n",
    "            temp_result[tick+'_'+model_name+\"_transactions\"] = transactions\n",
    "            ## Will contain total money for this model, including all stocks\n",
    "            total[model_name] += money\n",
    "            ## total transactions for the model, including all stocks\n",
    "            total[model_name+\"_transactions\"] += transactions          \n",
    "        \n",
    "        ## Calculate benchmark portfolio for current stock (NOT COMPLETE)\n",
    "        #temp_df = X['Close_pc'].to_frame()\n",
    "        #temp_df['Close_pc'].iloc[0] = 1       ## since first one is not played\n",
    "        #temp_df['Money'] = temp_df['Close_pc'].cumprod().mul(MONEY)\n",
    "        ### total money in benchmark, for that stock\n",
    "        #temp_result[tick+'_benchmark'] = float(format(temp_df['Money'].iloc[-1], '.2f'))                         \n",
    "        #total['benchmark'] += float(format(temp_df['Money'].iloc[-1], '.2f'))\n",
    "        \n",
    "        ## Calculate benchmark portfolio for current stock. SLOWER? BUT COMPLETE\n",
    "        money = copy.copy(MONEY)\n",
    "        for i,r in enumerate(X['Close_pc']):\n",
    "            if i > 0:\n",
    "                money = float(format(money*r,'.2f'))\n",
    "        temp_result[tick+'_benchmark'] = money                     ## total money in benchmark, for that stock\n",
    "        total['benchmark'] += money                                ## will contain total money in benchmark portfolio\n",
    "        total['benchmark_transcations'] = 2                        ## Initial buy and the sell at the end\n",
    "    \n",
    "    ## Determine Total values of each portfolio\n",
    "    temp_result['total_benchmark'] = total['benchmark']            ## total money in benchmark portfolio\n",
    "    \n",
    "    ## Get total money and transactions per each model.  Each including all stocks\n",
    "    for model_name,_ in models:\n",
    "        temp_result['total_'+model_name] = total[model_name]\n",
    "        temp_result['total_'+model_name+\"_transactions\"] = total[model_name+\"_transactions\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to save each of the testing stocks as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_testing_stocks(filename):\n",
    "    panel_data = pd.read_pickle(filename)                   ## read saved stocks data\n",
    "\n",
    "    ## Go through each stock\n",
    "    for tick in TESTING_TICKERS:\n",
    "        ## Extract single stock from panel_data\n",
    "        df = panel_data[:,:,tick]                                                   \n",
    "        df = get_stock_df(df,tick)\n",
    "        \n",
    "        ## Plot stock\n",
    "        plot_stock(tick,df)\n",
    "        \n",
    "        ## Save to csv\n",
    "        df.to_csv(tick+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to delete all .json files in the temp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_temp_contents():\n",
    "    filelist = glob.glob(\"temp/*.json\")\n",
    "    for f in filelist:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (57083, 89) Target shape: 57083\n",
      "Target found are: {1, -1}\n",
      "Sample of training data:\n",
      "Number of rows: 54228. Number of columns: 89.\n",
      "                   Vol   Open_pc   High_pc    Low_pc  Close_pc     r2_pc  \\\n",
      "Date                                                                       \n",
      "2012-09-13   1796979.0  0.998824  1.032144  0.983536  1.008624  1.004312   \n",
      "2016-05-31  27940040.0  1.001026  1.005386  0.998718  1.004104  1.002052   \n",
      "2015-02-02   1862540.0  1.005839  1.012947  0.955826  0.961158  0.980579   \n",
      "2014-04-08  25792130.0  1.006005  1.037689  1.001035  1.032098  1.016049   \n",
      "2017-01-06  31751900.0  1.001458  1.013292  0.998799  1.011148  1.005574   \n",
      "\n",
      "               r3_pc     r5_pc    r10_pc    r15_pc    ...     High_p40  \\\n",
      "Date                                                  ...                \n",
      "2012-09-13  1.010715  1.022971  1.074716  1.083863    ...     1.002084   \n",
      "2016-05-31  1.000085  0.995640  0.992229  0.997418    ...     1.012357   \n",
      "2015-02-02  0.981467  0.997563  1.004316  0.999001    ...     0.965033   \n",
      "2014-04-08  1.010009  1.030565  1.051356  1.095368    ...     0.844505   \n",
      "2017-01-06  1.002030  0.999074  0.999974  0.999634    ...     1.047771   \n",
      "\n",
      "             Low_p40  Close_p40    r2_p40    r3_p40    r5_p40   r10_p40  \\\n",
      "Date                                                                      \n",
      "2012-09-13  0.954891   0.979249  0.975062  0.981278  0.993178  1.043415   \n",
      "2016-05-31  1.005643   1.011066  1.009000  1.007020  1.002544  0.999109   \n",
      "2015-02-02  0.910614   0.915693  0.934195  0.935042  0.950376  0.956809   \n",
      "2014-04-08  0.814676   0.839955  0.826894  0.821979  0.838708  0.855628   \n",
      "2017-01-06  1.032785   1.045554  1.039790  1.036125  1.033069  1.034000   \n",
      "\n",
      "             r15_p40   r25_p40   r40_p40  \n",
      "Date                                      \n",
      "2012-09-13  1.052296  1.044405  1.001789  \n",
      "2016-05-31  1.004334  1.003153  0.999864  \n",
      "2015-02-02  0.951746  0.965565  0.997612  \n",
      "2014-04-08  0.891447  0.955136  0.995092  \n",
      "2017-01-06  1.033648  1.017175  1.001519  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "Epoch 1/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.9379 - acc: 0.5203     \n",
      "Epoch 2/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.6920 - acc: 0.5209     \n",
      "Epoch 3/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.4974 - acc: 0.5195     \n",
      "Epoch 4/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.3397 - acc: 0.5234     \n",
      "Epoch 5/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.2135 - acc: 0.5205     \n",
      "Epoch 6/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.1119 - acc: 0.5219     \n",
      "Epoch 7/20\n",
      "54228/54228 [==============================] - 1s - loss: 1.0299 - acc: 0.5209     \n",
      "Epoch 8/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.9645 - acc: 0.5211     \n",
      "Epoch 9/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.9116 - acc: 0.5230     \n",
      "Epoch 10/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.8686 - acc: 0.5244     \n",
      "Epoch 11/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.8348 - acc: 0.5199     \n",
      "Epoch 12/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.8071 - acc: 0.5238     \n",
      "Epoch 13/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7846 - acc: 0.5238     \n",
      "Epoch 14/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7670 - acc: 0.5197     \n",
      "Epoch 15/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7525 - acc: 0.5221     \n",
      "Epoch 16/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7405 - acc: 0.5245     \n",
      "Epoch 17/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7314 - acc: 0.5237     \n",
      "Epoch 18/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7240 - acc: 0.5203     \n",
      "Epoch 19/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7180 - acc: 0.5223     \n",
      "Epoch 20/20\n",
      "54228/54228 [==============================] - 1s - loss: 0.7132 - acc: 0.5214     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gio/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 481 ms, total: 18.1 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "## Remove all existing .json files in temp folder\n",
    "del_temp_contents()\n",
    "\n",
    "filename = 'training_stocks.pkl'\n",
    "## Generate data to be inputted to the models\n",
    "X = get_training_stocks_df(filename)\n",
    "\n",
    "filename = 'testing_stocks.pkl'\n",
    "## Generate data of test stocks and save to csv files\n",
    "save_testing_stocks(filename)\n",
    "\n",
    "## Check if multiprocessor is set\n",
    "if MULTIPROCESSOR == False:\n",
    "    ## For single processing\n",
    "    # Get performance of different types of models\n",
    "    for sell_below, buy_above in SELL_BUY_VALUES:\n",
    "        for target_ratio in TARGET_RATIOS:\n",
    "            get_model_metrics(target_ratio,sell_below,buy_above,X.copy())  ## copy to prevent updating\n",
    "            print(\"Completed target ratio: {} with sell: {} and buy: {}\".format(target_ratio,sell_below,buy_above))\n",
    "else:\n",
    "    ## For multiprocessing \n",
    "    ## Get the arguments for the pool\n",
    "    args = []\n",
    "    for sell_below, buy_above in SELL_BUY_VALUES:\n",
    "        for target_ratio in TARGET_RATIOS:\n",
    "            arg = (target_ratio,sell_below,buy_above,X.copy())\n",
    "            args.append(arg)\n",
    "\n",
    "    ## With multiprocessing\n",
    "    pool = mp.Pool(processes=NUM_PROCESSES)\n",
    "    pool.starmap(get_model_metrics,args)\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all json in temp folder and append to result (used for multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if MULTIPROCESSOR == True:\n",
    "    result = []\n",
    "\n",
    "    directory = \"temp/\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"): \n",
    "            js = open(directory+filename).read()\n",
    "            temp_dict = json.loads(js)\n",
    "            result.append(temp_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Adaboost Accuracy  Adaboost Fscore  BA_Adaboost  BA_Adaboost_transactions  \\\n",
      "0           0.926795         0.926811     16444.76                        20   \n",
      "\n",
      "   BA_DecisionTree  BA_DecisionTree_transactions  BA_GaussianNB  \\\n",
      "0         16031.71                            32       18139.25   \n",
      "\n",
      "   BA_GaussianNB_transactions     BA_NN  BA_NN_transactions       ...         \\\n",
      "0                           2  18139.25                   2       ...          \n",
      "\n",
      "   target_ratio  total_Adaboost  total_Adaboost_transactions  \\\n",
      "0       vr40_pc        93930.43                          198   \n",
      "\n",
      "   total_DecisionTree  total_DecisionTree_transactions  total_GaussianNB  \\\n",
      "0           100635.38                              330         101186.61   \n",
      "\n",
      "   total_GaussianNB_transactions   total_NN  total_NN_transactions  \\\n",
      "0                             22  101487.38                     18   \n",
      "\n",
      "   total_benchmark  \n",
      "0        101487.38  \n",
      "\n",
      "[1 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column FinalValue per model.  Which takes taxes & commisions into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Go through each row\n",
    "for index, row in result_df.iterrows():\n",
    "    ## Initialize params\n",
    "    benchmark = {'value':0,'loss':0,'gain':0,'transactions':len(TESTING_TICKERS)*2.0}\n",
    "    decisiontree = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    gaussiannb = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    adaboost = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    svc = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    nn = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    \n",
    "    #models_dict = {'benchmark':benchmark,'DecisionTree':decisiontree,'GaussianNB':gaussiannb,\\\n",
    "    #            'Adaboost':adaboost, 'SVC':svc, 'NN':nn}\n",
    "    ## No SVC model\n",
    "    models_dict = {'benchmark':benchmark,'DecisionTree':decisiontree,'GaussianNB':gaussiannb,\\\n",
    "                   'Adaboost':adaboost, 'NN':nn}\n",
    "    \n",
    "    ## Gather each stock information\n",
    "    for tick in TESTING_TICKERS:\n",
    "        ## update each models\n",
    "        for key, model in models_dict.items():\n",
    "            stock_value = row[tick+\"_\"+key]\n",
    "            if stock_value > MONEY:                         ## capital gain\n",
    "                model['gain'] += stock_value-MONEY\n",
    "            else:                                           ## loss\n",
    "                model['loss'] += MONEY-stock_value\n",
    "            \n",
    "            if key != 'benchmark':\n",
    "                model['transactions'] += row[tick+\"_\"+key+\"_transactions\"]\n",
    "        \n",
    "        \n",
    "    ## Get Final Values\n",
    "    for key, model in models_dict.items():\n",
    "        ## more gains than loss\n",
    "        if model['gain'] > model['loss']: \n",
    "            if key == 'benchmark':\n",
    "                model['value'] = float(format(row['total_'+key] - COMM_RATE*model['transactions'] \\\n",
    "                                                     - GAIN_LONG*(model['gain']-model['loss']),'.2f'))\n",
    "            else:\n",
    "                model['value'] = float(format(row['total_'+key] - COMM_RATE*model['transactions'] \\\n",
    "                                                     - GAIN_SHORT*(model['gain']-model['loss']),'.2f'))\n",
    "        ## more loss than gain\n",
    "        else:    \n",
    "            ## All model gains GAIN_LONG (assuming its on 25% tax bracket)\n",
    "            model['value'] = float(format(row['total_'+key] - COMM_RATE*model['transactions'] \\\n",
    "                                + GAIN_LONG*(model['loss']-model['gain']),'.2f')) ## add tax credit\n",
    "            \n",
    "\n",
    "        result_df.set_value(index,key+\"_FinalValue\",model['value'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Adaboost Accuracy  Adaboost Fscore  BA_Adaboost  BA_Adaboost_transactions  \\\n",
      "0           0.926795         0.926811     16444.76                        20   \n",
      "\n",
      "   BA_DecisionTree  BA_DecisionTree_transactions  BA_GaussianNB  \\\n",
      "0         16031.71                            32       18139.25   \n",
      "\n",
      "   BA_GaussianNB_transactions     BA_NN  BA_NN_transactions  \\\n",
      "0                           2  18139.25                   2   \n",
      "\n",
      "           ...           total_GaussianNB  total_GaussianNB_transactions  \\\n",
      "0          ...                  101186.61                             22   \n",
      "\n",
      "    total_NN  total_NN_transactions  total_benchmark  Adaboost_FinalValue  \\\n",
      "0  101487.38                     18        101487.38             91967.72   \n",
      "\n",
      "   NN_FinalValue  GaussianNB_FinalValue  DecisionTree_FinalValue  \\\n",
      "0       98526.43               98281.06                 96343.04   \n",
      "\n",
      "   benchmark_FinalValue  \n",
      "0              99675.17  \n",
      "\n",
      "[1 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result_df)\n",
    "result_df.to_csv('Results.csv')         ## can use parameters: mode='a', header=False, if memory is an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
