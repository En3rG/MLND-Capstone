{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gio/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Gio/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, make_scorer, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import pickle\n",
    "from numpy import array as np_array\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display                                 # Allows the use of display() for DataFrames\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import optimizers\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Start time.  Used to measure execution time.\n",
    "START = time.clock()\n",
    "\n",
    "DATA_SOURCE = 'google'\n",
    "\n",
    "## Date range used for training data\n",
    "TRAINING_START_DATE = '2010-01-01'\n",
    "TRAINING_END_DATE = '2017-7-31'\n",
    "\n",
    "## Date range used for testing data\n",
    "TEST_START_DATE = '2016-06-01'\n",
    "TEST_END_DATE = '2017-7-31'\n",
    "\n",
    "## Stock tickers for training data\n",
    "TRAINING_TICKERS = ['AAPL', 'GOOG', 'YHOO', 'T', 'IMAX', 'IBM', 'NFLX', 'SIRI', 'S', 'PLUG', \\\n",
    "           'C', 'BAC', 'P', 'NOK', 'XONE', 'SSYS', 'TSLA', 'AMZN', 'SDRL', 'DDD', \\\n",
    "           'DBO', 'SRPT', 'SPWR', 'SCTY', 'FB', 'URRE', 'NQ', 'TWTR', 'F', 'BAH', \\\n",
    "           'MZDAY', 'FSYS', 'BIDU', 'KORS', 'HLF', 'ORCL', 'MBLY']\n",
    "#TRAINING_TICKERS = ['AAPL', 'GOOG', 'YHOO','SSYS','SPWR','SDRL']\n",
    "#TRAINING_TICKERS = ['AAPL','GOOG']\n",
    "\n",
    "## Stock tickers for testing data\n",
    "#TESTING_TICKERS = ['SHOP', 'BA', 'SD', 'FCEL', 'HEMP', 'TPLM', 'CHK', 'OLED', 'HON', 'LMT', 'CMG', 'MA']\n",
    "TESTING_TICKERS = ['BA','OLED', 'HON','MA','TPLM', 'SD', 'FCEL', 'CHK', 'CMG']  ## unable to read SHOP,HEMP,LMT\n",
    "#TESTING_TICKERS = ['AAPL','GOOG']\n",
    "#TESTING_TICKERS = ['SD', 'FCEL']\n",
    "#TESTING_TICKERS = ['BA','OLED', 'HON','MA']\n",
    "#TESTING_TICKERS = ['SD', 'FCEL', 'CHK', 'CMG']\n",
    "\n",
    "## Initial money to be invested\n",
    "MONEY = 10000\n",
    "\n",
    "## Commision rate when buying/selling stocks\n",
    "COMM_RATE = 4.95\n",
    "\n",
    "## Long term capital gain tax rate (percentage)\n",
    "GAIN_LONG = 0.15\n",
    "\n",
    "## Short term capital gain tax rate (percentage).  Also used for losses, assuming its the individuals tax bracket\n",
    "GAIN_SHORT = 0.25\n",
    "\n",
    "## Models predict the ratio base on these targets\n",
    "#TARGET_RATIOS = ['vClose_pc','vr2_pc','vr3_pc','vr5_pc','vr10_pc','vr15_pc','vr25_pc','vr40_pc']\n",
    "TARGET_RATIOS = ['vClose_pc','vr2_pc']\n",
    "\n",
    "## Used to determine when predictions will be a buy/sell\n",
    "SELL_BUY_VALUES = [(0.985,0.985), (1,1)]\n",
    "#SELL_BUY_VALUES = [(0.99,0.99), (1,1),(0.98,1.02),(0.99,1.01),(1.00,1.02),(1.00,1.03)]\n",
    "# SELL_BUY_VALUES = [(0.985,0.985),(0.99,0.99),(0.995,0.995), (1,1),(1.005,1.005),(1.01,1.01),(1.015,1.015), \\\n",
    "#                    (0.985,1.015),(0.99,1.01),(0.995,1.005),\\\n",
    "#                    (0.995,1.00),(1.00,1.005),(1.005,1.01), \\\n",
    "#                    (1.00,1.015),(1.00,1.01)]\n",
    "\n",
    "## Set to True if we are reading existing models\n",
    "## Set to False to generate new models\n",
    "READ_MODEL = False\n",
    "\n",
    "## Number of processes for multiprocessing pool\n",
    "NUM_PROCESSES = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the stocks data.  Save data as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_training_data():\n",
    "    all_weekdays = pd.date_range(start=TRAINING_START_DATE,end=TRAINING_END_DATE,freq='B')\n",
    "    panel_data = data.DataReader(TRAINING_TICKERS,DATA_SOURCE,TRAINING_START_DATE,TRAINING_END_DATE) ## panel type\n",
    "    panel_data.to_pickle('training_stocks.pkl')                                                      ## save to pickle\n",
    "\n",
    "def gather_testing_data():\n",
    "    all_weekdays = pd.date_range(start=TEST_START_DATE,end=TEST_END_DATE,freq='B')\n",
    "    panel_data = data.DataReader(TESTING_TICKERS,DATA_SOURCE,TEST_START_DATE,TEST_END_DATE)           \n",
    "    panel_data.to_pickle('testing_stocks.pkl')  \n",
    "    \n",
    "gather_training_data()                                           ## can comment out if pickle file is already locally\n",
    "gather_testing_data()                                            ## can comment out if pickle file is already locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get the rolling averages. 2, 3, 5, 10, 15, 25 and 40-day moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rolling(df):\n",
    "    stock = df['Close']\n",
    "    r2 = stock.rolling(window=2).mean()\n",
    "    r3 = stock.rolling(window=3).mean()\n",
    "    r5 = stock.rolling(window=5).mean()\n",
    "    r10 = stock.rolling(window=10).mean()\n",
    "    r15 = stock.rolling(window=15).mean()\n",
    "    r25 = stock.rolling(window=25).mean()\n",
    "    r40 = stock.rolling(window=40).mean()\n",
    "\n",
    "    return r2, r3, r5, r10, r15, r25, r40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stock(tick, df):\n",
    "    close = df['vClose_pc']\n",
    "    r2 = df['vr2_pc']\n",
    "    r3 = df['vr3_pc']\n",
    "    r5 = df['vr5_pc']\n",
    "    r10 = df['vr10_pc']\n",
    "    r15 = df['vr15_pc']\n",
    "    r25 = df['vr25_pc']\n",
    "    r40 = df['vr40_pc']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(close.index,close,label=tick)\n",
    "    ax.plot(r2.index, r2, label='2 days rolling')\n",
    "    ax.plot(r3.index, r3, label='3 days rolling')\n",
    "    ax.plot(r5.index, r5, label='5 days rolling')\n",
    "    ax.plot(r10.index, r10, label='10 days rolling')\n",
    "    ax.plot(r15.index, r15, label='15 days rolling')\n",
    "    ax.plot(r25.index, r25, label='25 days rolling')\n",
    "    ax.plot(r40.index, r40, label='40 days rolling')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Closing prices ($)')\n",
    "    ax.legend()\n",
    "\n",
    "    #plt.show()\n",
    "    fig.savefig(\"figures/\"+tick+'.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a DataFrame of the entire list of stocks for training, from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_stocks_df(filename):\n",
    "    main_df = pd.DataFrame()\n",
    "    dfs = []\n",
    "    panel_data = pd.read_pickle(filename)                                           ## read saved stocks data\n",
    "\n",
    "    for tick in TRAINING_TICKERS:\n",
    "        ## Extract single stock from panel_data\n",
    "        df = panel_data[:,:,tick]                                                   ## becomes df type, from panel\n",
    "        df = get_stock_df(df,tick)\n",
    "        \n",
    "        ## Plot stock\n",
    "        plot_stock(tick,df)\n",
    "        \n",
    "        #main_df = main_df.append(df)                                     \n",
    "        dfs.append(df)                                           ## faster to append once, with [] of df\n",
    "\n",
    "    main_df = main_df.append(dfs)                                ## faster to append once, with [] of df\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get DataFrame of individual stocks.  Generates 106 columns, 17 columns will be removed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List columns generated here:\n",
    "\n",
    "\"\"\"\n",
    "def get_stock_df(df,tick):\n",
    "    r2, r3, r5, r10, r15, r25, r40 = get_rolling(df)                            ## get moving averages\n",
    "\n",
    "    ## Rename Volume column then remove it\n",
    "    vol = df['Volume']\n",
    "    vol = vol.to_frame()\n",
    "    vol.columns = ['Vol']\n",
    "    df = df.drop('Volume', 1)                                                   ## 1 for axis 1, which is column\n",
    "\n",
    "    ## Rename columns\n",
    "    df_r2= r2.to_frame()                                                        ## from series to df\n",
    "    df_r2.columns = ['r2']                                                      ## change column title\n",
    "\n",
    "    df_r3= r3.to_frame()\n",
    "    df_r3.columns = ['r3']\n",
    "\n",
    "    df_r5= r5.to_frame()\n",
    "    df_r5.columns = ['r5']\n",
    "\n",
    "    df_r10= r10.to_frame()                                                      \n",
    "    df_r10.columns = ['r10']                                                    \n",
    "\n",
    "    df_r15= r15.to_frame()\n",
    "    df_r15.columns = ['r15']\n",
    "\n",
    "    df_r25= r25.to_frame()\n",
    "    df_r25.columns = ['r25']\n",
    "\n",
    "    df_r40= r40.to_frame()\n",
    "    df_r40.columns = ['r40']\n",
    "\n",
    "    ## Shift rows to generate next/previous values\n",
    "    next_close = df['Close'].shift(-1)                                          ## a series\n",
    "    next_close = next_close.to_frame()\n",
    "    next_close.columns = ['next_close']\n",
    "\n",
    "    prev_close = df['Close'].shift(1)\n",
    "    prev_close = prev_close.to_frame()\n",
    "    prev_close.columns = ['prev_close']\n",
    "\n",
    "    prev_r2 = df_r2['r2'].shift(1)\n",
    "    prev_r2 = prev_r2.to_frame()\n",
    "    prev_r2.columns = ['prev_r2']\n",
    "\n",
    "    prev_r3 = df_r3['r3'].shift(1)\n",
    "    prev_r3 = prev_r3.to_frame()\n",
    "    prev_r3.columns = ['prev_r3']\n",
    "\n",
    "    prev_r5 = df_r5['r5'].shift(1)\n",
    "    prev_r5 = prev_r5.to_frame()\n",
    "    prev_r5.columns = ['prev_r5']\n",
    "\n",
    "    prev_r10 = df_r10['r10'].shift(1)\n",
    "    prev_r10 = prev_r10.to_frame()\n",
    "    prev_r10.columns = ['prev_r10']\n",
    "\n",
    "    prev_r15 = df_r15['r15'].shift(1)\n",
    "    prev_r15 = prev_r15.to_frame()\n",
    "    prev_r15.columns = ['prev_r15']\n",
    "\n",
    "    prev_r25 = df_r25['r25'].shift(1)\n",
    "    prev_r25 = prev_r25.to_frame()\n",
    "    prev_r25.columns = ['prev_r25']\n",
    "\n",
    "    prev_r40 = df_r40['r40'].shift(1)\n",
    "    prev_r40 = prev_r40.to_frame()\n",
    "    prev_r40.columns = ['prev_r40']\n",
    "\n",
    "    ## Generate entire dataframe\n",
    "    ## encapsulate in a list for multiple df\n",
    "    df1 = next_close.join([prev_close,prev_r2,prev_r3,prev_r5,prev_r10,prev_r15,prev_r25,prev_r40,vol]) \n",
    "    df2 = df.join([df_r2,df_r3,df_r5,df_r10,df_r15, df_r25, df_r40])\n",
    "    df3 = df2.copy()\n",
    "    df4 = df2.copy()\n",
    "    df5 = df2.copy()\n",
    "    df6 = df2.copy()\n",
    "    df6 = df2.copy()\n",
    "    df7 = df2.copy()\n",
    "    df8 = df2.copy()\n",
    "    df9 = df2.copy()\n",
    "    df10 = df2.copy()\n",
    "    ## will have original value (not percentage)\n",
    "    df10.columns = ['vOpen_pc','vHigh_pc','vLow_pc','vClose_pc','vr2_pc','vr3_pc','vr5_pc','vr10_pc','vr15_pc','vr25_pc','vr40_pc']\n",
    "    ## will be with respect to prev_close\n",
    "    df2.columns = ['Open_pc','High_pc','Low_pc','Close_pc','r2_pc','r3_pc','r5_pc','r10_pc','r15_pc','r25_pc','r40_pc'] \n",
    "    ## will be with respect to prev_r2\n",
    "    df3.columns = ['Open_p2','High_p2','Low_p2','Close_p2','r2_p2','r3_p2','r5_p2','r10_p2','r15_p2','r25_p2','r40_p2']     \n",
    "    ## will be with respect to prev_r3\n",
    "    df4.columns = ['Open_p3','High_p3','Low_p3','Close_p3','r2_p3','r3_p3','r5_p3','r10_p3','r15_p3','r25_p3','r40_p3']     \n",
    "    ## will be with respect to prev_r5\n",
    "    df5.columns = ['Open_p5','High_p5','Low_p5','Close_p5','r2_p5','r3_p5','r5_p5','r10_p5','r15_p5','r25_p5','r40_p5']     \n",
    "    ## will be with respect to prev_r10\n",
    "    df6.columns = ['Open_p10','High_p10','Low_p10','Close_p10','r2_p10','r3_p10','r5_p10','r10_p10','r15_p10','r25_p10','r40_p10']     \n",
    "    ## will be with respect to prev_r15\n",
    "    df7.columns = ['Open_p15','High_p15','Low_p15','Close_p15','r2_p15','r3_p15','r5_p15','r10_p15','r15_p15','r25_p15','r40_p15']     \n",
    "    ## will be with respect to prev_r25\n",
    "    df8.columns = ['Open_p25','High_p25','Low_p25','Close_p25','r2_p25','r3_p25','r5_p25','r10_p25','r15_p25','r25_p25','r40_p25']     \n",
    "    ## will be with respect to prev_r40\n",
    "    df9.columns = ['Open_p40','High_p40','Low_p40','Close_p40','r2_p40','r3_p40','r5_p40','r10_p40','r15_p40','r25_p40','r40_p40']     \n",
    "\n",
    "    ## Combine all to one dataframe\n",
    "    df = df1.join([df10,df2,df3,df4,df5,df6,df7,df8,df9])\n",
    "\n",
    "    ## Drop N/A\n",
    "    df = df.dropna(axis=0,how='any')                                            ## drop rows containing at least 1 NA\n",
    "\n",
    "    ## Normalize columns, base on previous values.  Get ratios/percentage \n",
    "    #df[columns_to_divide] = df[columns_to_divide] / df['prev_close']           ## having size issues\n",
    "\n",
    "    cols_to_divide = ['Open_pc','High_pc','Low_pc','Close_pc','r2_pc','r3_pc','r5_pc','r10_pc','r15_pc','r25_pc','r40_pc']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_close'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p2','High_p2','Low_p2','Close_p2','r2_p2','r3_p2','r5_p2','r10_p2','r15_p2','r25_p2','r40_p2']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r2'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p3','High_p3','Low_p3','Close_p3','r2_p3','r3_p3','r5_p3','r10_p3','r15_p3','r25_p3','r40_p3']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r3'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p5','High_p5','Low_p5','Close_p5','r2_p5','r3_p5','r5_p5','r10_p5','r15_p5','r25_p5','r40_p5']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r5'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p10','High_p10','Low_p10','Close_p10','r2_p10','r3_p10','r5_p10','r10_p10','r15_p10','r25_p10','r40_p10']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r10'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p15','High_p15','Low_p15','Close_p15','r2_p15','r3_p15','r5_p15','r10_p15','r15_p15','r25_p15','r40_p15']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r15'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p25','High_p25','Low_p25','Close_p25','r2_p25','r3_p25','r5_p25','r10_p25','r15_p25','r25_p25','r40_p25']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r25'].values,axis=0)\n",
    "\n",
    "    cols_to_divide = ['Open_p40','High_p40','Low_p40','Close_p40','r2_p40','r3_p40','r5_p40','r10_p40','r15_p40','r25_p40','r40_p40']\n",
    "    df[cols_to_divide] = df[cols_to_divide].div(df['prev_r40'].values,axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update target, base on target ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target(X,target_ratio):\n",
    "    ## update next_close base on target_ratio\n",
    "    cols_to_divide = ['next_close']\n",
    "    X[cols_to_divide] = X[cols_to_divide].div(X[target_ratio].values,axis=0)\n",
    "    \n",
    "    ## Convert target base on sell/buy prices thats provided\n",
    "    y = convert_target_value(X['next_close'],sell_below,buy_above)\n",
    "\n",
    "    ## Delete unneccesary columns\n",
    "    del X['next_close'],X['prev_close'],X['prev_r2'],X['prev_r3'],X['prev_r5']\n",
    "    del X['prev_r10'],X['prev_r15'],X['prev_r25'],X['prev_r40']\n",
    "    ## Delete columns with actual price\n",
    "    del X['vOpen_pc'],X['vHigh_pc'],X['vLow_pc'],X['vClose_pc'],X['vr2_pc']\n",
    "    del X['vr3_pc'],X['vr5_pc'],X['vr10_pc'],X['vr15_pc'],X['vr25_pc'],X['vr40_pc']\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to convert target.  Buy (1), Sell (-1), or Neutral (0).  Base on sell/buy prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_target_value(arr,sell_below,buy_above):\n",
    "    ans = []\n",
    "\n",
    "    for x in arr:\n",
    "        if x <= sell_below:\n",
    "            ans.append(-1)\n",
    "        elif x >= buy_above:\n",
    "            ans.append(1)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model():\n",
    "    ## create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500,input_dim=89,activation='relu'))\n",
    "    model.add(Dense(300,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(3,activation='softmax'))   \n",
    "    \n",
    "    #model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])        ## for binary classifier\n",
    "    #model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adamax',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "#     opt = optimizers.SGD(lr=0.1,momentum=0.9,decay=1e-6,nesterov=True)\n",
    "#     model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to get encoding, for target in Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a (XXX,3) from (XXX,1), for the input of the neural network\n",
    "\"\"\"\n",
    "def myEncoder(arr):\n",
    "    s = (len(arr),3)\n",
    "    numpy_arr = np.zeros(s)\n",
    "    \n",
    "    for i,num in enumerate(arr):\n",
    "        if num == -1:\n",
    "            numpy_arr[i] = np_array([1,0,0])\n",
    "        elif num == 1:\n",
    "            numpy_arr[i] = np_array([0,0,1])\n",
    "        else:\n",
    "            numpy_arr[i] = np_array([0,1,0])\n",
    "\n",
    "    return numpy_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to decode, as 1 dimensional target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a (XXX,1) from (XXX,3), from predict of the neural network\n",
    "\"\"\"\n",
    "def myDecoder(numpy_arr):\n",
    "    arr = []\n",
    "    for i,a in enumerate(numpy_arr):\n",
    "        ## Its not really necessary to create an array, since its only 1D\n",
    "        neg_one = np_array([1,0,0])\n",
    "        zero = np_array([0,1,0])\n",
    "        pos_one = np_array([0,0,1])\n",
    "        \n",
    "        ## if a == neg_one:   <-- This gives an ambigious error\n",
    "        if all(x == y for x, y in zip(a,neg_one)):\n",
    "            arr.append(-1)\n",
    "        elif all(x == y for x, y in zip(a,pos_one)):\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "        \n",
    "    ## Convert the arr list to an ndarray.  Models predictions are ndarray \n",
    "    return np_array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpter function to normalize input data for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale_input(arr):\n",
    "    \"\"\"\n",
    "    Scales data to be between a - b\n",
    "    \"\"\"\n",
    "    ## Function below is not used.  Initially thought arr was a df\n",
    "    def scaler(x):\n",
    "        new_x = (((b-a)*(x-minimum))/(maximum-minimum))+a\n",
    "        print(\"testing\",x,new_x)\n",
    "        return new_x\n",
    "        \n",
    "    a = -1\n",
    "    b = 1\n",
    "    maximum = 1.5\n",
    "    minimum = 0.5\n",
    "    \n",
    "    #result = arr.applymap(scaler)    ## if its a df, use applymap\n",
    "    \n",
    "    ## arr is an ndarray.  Use vectorize instead of applymap\n",
    "    scaler = lambda x: (((b-a)*(x-minimum))/(maximum-minimum))+a\n",
    "    vfunc = np.vectorize(scaler)\n",
    "    result = vfunc(arr)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the metrics of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_metrics(target_ratio,sell_below,buy_above,X):\n",
    "    temp_result = {}\n",
    "    temp_result['target_ratio'] = target_ratio\n",
    "    temp_result['sell_below'] = sell_below\n",
    "    temp_result['buy_above'] = buy_above\n",
    "    \n",
    "    X,y = get_target(X,target_ratio)\n",
    "    num_targets = set(y)\n",
    "    print(\"Target found are: {}\".format(num_targets))\n",
    "    \n",
    "    ## Perform cross validation.  95% to have as much training data as possible.  \n",
    "    ## Also, performance will be base on totally different set of stocks\n",
    "    try: X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,train_size=0.95,stratify=y)\n",
    "    ## Getting error sometimes: The least populated class in y has only 1 member, which is too few. \n",
    "    ## The minimum number of labels for any class cannot be less than 2.\n",
    "    except: X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,train_size=0.95)\n",
    "        \n",
    "    print(\"Sample of training data:\")\n",
    "    print(\"Number of rows: {}. Number of columns: {}.\".format(len(X_train),len(X_train.columns)))\n",
    "    print(X_train.head())\n",
    "    \n",
    "    beta = 0.5\n",
    "    \n",
    "    ## Initialize Models\n",
    "    clf1 = DecisionTreeClassifier()\n",
    "    clf2 = GaussianNB()\n",
    "    clf3 = SVC(random_state=0)\n",
    "    clf4 = AdaBoostClassifier(random_state=0)\n",
    "    clf5 = neural_network_model()\n",
    "    \n",
    "    if READ_MODEL == True:\n",
    "        ## Read existing models\n",
    "        \n",
    "        ## For DecisionTree\n",
    "        with open(\"models/DecisionTree_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf1 = pickle.load(f)\n",
    "            \n",
    "        ## For GaussianNB\n",
    "        with open(\"models/GaussianNB_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf2 = pickle.load(f)\n",
    "    \n",
    "        ## For SVC\n",
    "        with open(\"models/SVC_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf3 = pickle.load(f)\n",
    "            \n",
    "        ## For Adaboost\n",
    "        with open(\"models/Adaboost_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'rb') as f:\n",
    "            clf4 = pickle.load(f)\n",
    "          \n",
    "        ## For Neural Network\n",
    "        ## Load json and create model\n",
    "        json_file = open(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".json\",\"r\")\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        clf5 = model_from_json(loaded_model_json)\n",
    "        ## Load weights into new model\n",
    "        clf5.load_weights(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".h5\")\n",
    "\n",
    "    \n",
    "    else:\n",
    "        ## Generate new models\n",
    "        \n",
    "        ## Fit Data to DecisionTree Model\n",
    "        clf1.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/DecisionTree_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf1, f)\n",
    "\n",
    "        # Fit Data to GaussianNB Model\n",
    "        clf2.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/GaussianNB_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf2, f)\n",
    "        \n",
    "        # Fit Data to SVC Model\n",
    "        clf3.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/SVC_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf3, f)\n",
    "\n",
    "        # Fit Data to Adaboost Model\n",
    "        clf4.fit(X_train,y_train)\n",
    "        ## Save model to a file\n",
    "        with open(\"models/Adaboost_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(clf4, f)\n",
    "    \n",
    "        ## Fit Data to Neural Model\n",
    "        input_train = X_train.as_matrix(columns=None)                     ## convert df to numpy array\n",
    "        input_train = rescale_input(input_train)\n",
    "        #print(\"Input_train:\",input_train[np.r_[0:5]])                    ## print first 5\n",
    "        \n",
    "        ### encode class values as integers \n",
    "        #encoder = LabelEncoder()\n",
    "        #encoder.fit(y_train)\n",
    "        #encoded_y = encoder.transform(y_train)\n",
    "        ### convert integers to dummy variables (i.e one hot encoded)\n",
    "        #dummy_y = np_utils.to_categorical(encoded_y)\n",
    "        #print(dummy_y)\n",
    "        dummy_y = myEncoder(y_train)\n",
    "        clf5.fit(input_train,dummy_y,epochs=10,batch_size=50)  \n",
    "        ## Serialize model to JSON\n",
    "        model_json = clf5.to_json()\n",
    "        with open(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".json\",\"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        ## Serialize weights to HDF5\n",
    "        clf5.save_weights(\"models/NN_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".h5\")\n",
    "    \n",
    "    ## Get accuracy and fscore of the models \n",
    "    temp_result['DecisionTree Accuracy'] = accuracy_score(y_test, clf1.predict(X_test))\n",
    "    temp_result['DecisionTree Fscore'] = fbeta_score(y_test, clf1.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    temp_result['GaussianNB Accuracy'] = accuracy_score(y_test,clf2.predict(X_test))\n",
    "    temp_result['GaussianNB Fscore'] = fbeta_score(y_test, clf2.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    temp_result['SVC Accuracy'] = accuracy_score(y_test,clf3.predict(X_test))\n",
    "    temp_result['SVC Fscore'] = fbeta_score(y_test, clf3.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    temp_result['Adaboost Accuracy'] = accuracy_score(y_test,clf4.predict(X_test))\n",
    "    temp_result['Adaboost Fscore'] = fbeta_score(y_test, clf4.predict(X_test),beta,average='weighted')\n",
    "\n",
    "    input_test = X_test.as_matrix(columns=None)\n",
    "    input_test = rescale_input(input_test)\n",
    "    dummy_y = myEncoder(y_test)\n",
    "    scores = clf5.evaluate(input_test, dummy_y)\n",
    "    temp_result['NN Accuracy'] = scores[1]*100\n",
    "    #temp_result['NN Accuracy'] = accuracy_score(dummy_y,clf5.predict(input_test))\n",
    "    #temp_result['NN Fscore'] = fbeta_score(dummy_y, clf5.predict(input_test),beta,average='weighted')\n",
    "\n",
    "    ## Determine performance of the portfolio on each model\n",
    "    models = [('DecisionTree',clf1),('GaussianNB',clf2),('Adaboost',clf4),('SVC',clf3),('NN',clf5)]\n",
    "    test_model_performance(models,temp_result,target_ratio,sell_below,buy_above)\n",
    "    \n",
    "    ## Result will contain all the result from each combination of the models\n",
    "    #result.append(temp_result)   <-- used before doing multiprocessing\n",
    "    ## To support multiprocessing, will have to save for later\n",
    "    #temp_result.to_csv(\"temp/\"+str(time.clock())+\".csv\")   <-- temp_result is a dict, not df\n",
    "    with open(\"temp/\"+str(time.clock())+\".json\", 'w') as fp:\n",
    "        json.dump(temp_result, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_performance(models,temp_result,target_ratio,sell_below,buy_above):\n",
    "    panel_data = pd.read_pickle('testing_stocks.pkl')                   ## read saved stocks data\n",
    "\n",
    "    ## Initialize total to be 0 across all models\n",
    "    total = {'benchmark':0}\n",
    "    for model_name,_ in models:\n",
    "        total[model_name] = 0                                           ## will hold total money for that model\n",
    "        total[model_name+\"_transactions\"] = 0                           ## will hold total transactions for that model\n",
    "    \n",
    "    ## Go through each stock\n",
    "    for tick in TESTING_TICKERS:\n",
    "        ## Extract single stock from panel_data\n",
    "        #df = panel_data[:,:,tick]                                                   \n",
    "        #X = get_stock_df(df,tick)\n",
    "        ## We will now just read the test stocks, generated before\n",
    "        X = pd.DataFrame.from_csv(tick+'.csv')\n",
    "        \n",
    "        ## Delete unnecessary columns\n",
    "        del X['next_close'],X['prev_close'],X['prev_r2'],X['prev_r3'],X['prev_r5']\n",
    "        del X['prev_r10'],X['prev_r15'],X['prev_r25'],X['prev_r40']\n",
    "        ## Delete columns with actual price\n",
    "        del X['vOpen_pc'],X['vHigh_pc'],X['vLow_pc'],X['vClose_pc'],X['vr2_pc']\n",
    "        del X['vr3_pc'],X['vr5_pc'],X['vr10_pc'],X['vr15_pc'],X['vr25_pc'],X['vr40_pc']\n",
    "        \n",
    "        ## Get predictions base on each models\n",
    "        for model_name, model in models:\n",
    "            ## predictions will be an ndarray\n",
    "            if model_name == 'NN':\n",
    "                input_test = X.as_matrix(columns=None)\n",
    "                input_test = rescale_input(input_test)\n",
    "                predictions = model.predict(input_test)\n",
    "                predictions = myDecoder(predictions)\n",
    "            else:\n",
    "                predictions = model.predict(X)         \n",
    "                        \n",
    "            ## Add predictions to the dataframe\n",
    "            pred = pd.DataFrame(predictions.flatten(),index=X.index,columns=['Predictions'])\n",
    "            S = X.join(pred)\n",
    "            S['Transactions'] = 0                                 ## will contain number of transactions\n",
    "            S['Money'] = 0                                        ## will contain total current money amount\n",
    "            \n",
    "            ## Calculate transactions (NOT COMPLETE)\n",
    "            #temp_df = pd.DataFrame(X['Close_pc'].values,columns=['Close_pc'])\n",
    "            #temp_df = temp_df.join(pd.DataFrame(predictions.flatten(),columns=['Predictions']))\n",
    "            #temp_df['playing'] = temp_df['Predictions'].shift().eq(1)\n",
    "            ### cumprod of 'Close_pc' where 'playing' is True.  Then multiple with initial money\n",
    "            #temp_df['Money'] = temp_df['Close_pc'].where(temp_df['playing'],1).cumprod().mul(MONEY)\n",
    "            ### get just last value from 'Money'\n",
    "            #temp_result[tick+'_'+model_name] = float(format(temp_df['Money'].iloc[-1], '.2f'))           \n",
    "            #total[model_name] += float(format(temp_df['Money'].iloc[-1], '.2f'))\n",
    "            \n",
    "            ## Calculate transactions. SLOWER? BUT COMPLETE\n",
    "            playing = False                                        ## used to determine if currently in the market\n",
    "            money = copy.copy(MONEY)\n",
    "            transactions = 0\n",
    "            i = 0\n",
    "            for index, row in S.iterrows():\n",
    "                ## Update money\n",
    "                if i > 0 and playing == True:\n",
    "                    money = float(format(money*row['Close_pc'],'.2f'))\n",
    "                \n",
    "                ## Buy/Sell\n",
    "                if row['Predictions'] == 1:\n",
    "                    if playing == False:\n",
    "                        playing = True                            ## Buy, playing after this\n",
    "                        transactions += 1                         ## increment transaction number for this model\n",
    "                elif row['Predictions'] == -1:\n",
    "                    if playing == True:\n",
    "                        playing = False                           ## Sell, not playing after this\n",
    "                        transactions += 1                         ## increment transaction number for this model\n",
    "                i += 1        \n",
    "                \n",
    "                ## Change value in sample data S in index and column provided, with value/data provided\n",
    "                S.set_value(index,'Money',money)\n",
    "                S.set_value(index,'Transactions',transactions)\n",
    "                \n",
    "            ## Save dataframe for testing purposes\n",
    "            S.to_csv(\"data/\"+tick+\"_\"+model_name+\"_\"+target_ratio+\"_\"+str(sell_below)+\"_\"+str(buy_above)+\".csv\")\n",
    "                    \n",
    "            ## If still playing at the end, we'll sell, thus increment number of transactions\n",
    "            transactions = transactions + 1 if playing == True else transactions\n",
    "            ## Will contain total money for this stock and model\n",
    "            temp_result[tick+'_'+model_name] = money\n",
    "            ## Will contain total number of transactions for this stock and model\n",
    "            temp_result[tick+'_'+model_name+\"_transactions\"] = transactions\n",
    "            ## Will contain total money for this model, including all stocks\n",
    "            total[model_name] += money\n",
    "            ## total transactions for the model, including all stocks\n",
    "            total[model_name+\"_transactions\"] += transactions          \n",
    "        \n",
    "        ## Calculate benchmark portfolio for current stock (NOT COMPLETE)\n",
    "        #temp_df = X['Close_pc'].to_frame()\n",
    "        #temp_df['Close_pc'].iloc[0] = 1       ## since first one is not played\n",
    "        #temp_df['Money'] = temp_df['Close_pc'].cumprod().mul(MONEY)\n",
    "        ### total money in benchmark, for that stock\n",
    "        #temp_result[tick+'_benchmark'] = float(format(temp_df['Money'].iloc[-1], '.2f'))                         \n",
    "        #total['benchmark'] += float(format(temp_df['Money'].iloc[-1], '.2f'))\n",
    "        \n",
    "        ## Calculate benchmark portfolio for current stock. SLOWER? BUT COMPLETE\n",
    "        money = copy.copy(MONEY)\n",
    "        for i,r in enumerate(X['Close_pc']):\n",
    "            if i > 0:\n",
    "                money = float(format(money*r,'.2f'))\n",
    "        temp_result[tick+'_benchmark'] = money                     ## total money in benchmark, for that stock\n",
    "        total['benchmark'] += money                                ## will contain total money in benchmark portfolio\n",
    "        total['benchmark_transcations'] = 2                        ## Initial buy and the sell at the end\n",
    "    \n",
    "    ## Determine Total values of each portfolio\n",
    "    temp_result['total_benchmark'] = total['benchmark']            ## total money in benchmark portfolio\n",
    "    \n",
    "    ## Get total money and transactions per each model.  Each including all stocks\n",
    "    for model_name,_ in models:\n",
    "        temp_result['total_'+model_name] = total[model_name]\n",
    "        temp_result['total_'+model_name+\"_transactions\"] = total[model_name+\"_transactions\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to save each of the testing stocks as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_testing_stocks(filename):\n",
    "    panel_data = pd.read_pickle(filename)                   ## read saved stocks data\n",
    "\n",
    "    ## Go through each stock\n",
    "    for tick in TESTING_TICKERS:\n",
    "        ## Extract single stock from panel_data\n",
    "        df = panel_data[:,:,tick]                                                   \n",
    "        df = get_stock_df(df,tick)\n",
    "        \n",
    "        ## Plot stock\n",
    "        plot_stock(tick,df)\n",
    "        \n",
    "        ## Save to csv\n",
    "        df.to_csv(tick+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target found are: {1, -1}\n",
      "Sample of training data:\n",
      "Number of rows: 54228. Number of columns: 89.\n",
      "                   Vol   Open_pc   High_pc    Low_pc  Close_pc     r2_pc  \\\n",
      "Date                                                                       \n",
      "2012-05-30    256616.0  0.988334  0.996760  0.950097  0.971484  0.985742   \n",
      "2014-11-13   3686428.0  1.001573  1.020481  1.000000  1.015955  1.007977   \n",
      "2010-12-16  10421479.0  1.000000  1.006085  0.993915  1.005071  1.002535   \n",
      "2014-11-13   8477486.0  0.983190  0.983190  0.919383  0.963293  0.981647   \n",
      "2014-08-13  29265662.0  1.003982  1.019497  1.003021  1.012907  1.006453   \n",
      "\n",
      "               r3_pc     r5_pc    r10_pc    r15_pc    ...     High_p40  \\\n",
      "Date                                                  ...                \n",
      "2012-05-30  0.983366  0.992223  1.001685  1.023979    ...     0.775348   \n",
      "2014-11-13  1.005853  0.991923  0.979782  0.966888    ...     1.021688   \n",
      "2010-12-16  1.004733  1.003854  1.007201  0.992563    ...     0.966202   \n",
      "2014-11-13  0.990280  1.010086  1.041852  1.042653    ...     0.904408   \n",
      "2014-08-13  1.007094  1.005822  1.002266  1.010051    ...     1.075467   \n",
      "\n",
      "             Low_p40  Close_p40    r2_p40    r3_p40    r5_p40   r10_p40  \\\n",
      "Date                                                                      \n",
      "2012-05-30  0.739051   0.755687  0.766778  0.764930  0.771819  0.779180   \n",
      "2014-11-13  1.001183   1.017156  1.009169  1.007043  0.993096  0.980941   \n",
      "2010-12-16  0.954514   0.965228  0.962793  0.964904  0.964060  0.967274   \n",
      "2014-11-13  0.845713   0.886105  0.902988  0.910929  0.929148  0.958369   \n",
      "2014-08-13  1.058086   1.068515  1.061707  1.062383  1.061041  1.057289   \n",
      "\n",
      "             r15_p40   r25_p40   r40_p40  \n",
      "Date                                      \n",
      "2012-05-30  0.796522  0.896036  0.987750  \n",
      "2014-11-13  0.968032  0.977591  0.999315  \n",
      "2010-12-16  0.953216  0.965501  0.997760  \n",
      "2014-11-13  0.959105  0.945923  0.992466  \n",
      "2014-08-13  1.065502  1.032536  1.003393  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "Target found are: {1, -1}\n",
      "Sample of training data:\n",
      "Number of rows: 54228. Number of columns: 89.\n",
      "                   Vol   Open_pc   High_pc    Low_pc  Close_pc     r2_pc  \\\n",
      "Date                                                                       \n",
      "2016-12-21  61079086.0  1.000440  1.000440  0.989432  0.996477  0.998239   \n",
      "2010-09-23  14892838.0  0.992158  1.036597  0.986712  1.035072  1.017536   \n",
      "2015-07-01   4408307.0  1.012830  1.033262  1.009979  1.024234  1.012117   \n",
      "2013-12-12  30946095.0  0.978877  0.980903  0.963252  0.972222  0.986111   \n",
      "2011-08-10  99097088.0  0.974544  0.974859  0.890949  0.895349  0.947674   \n",
      "\n",
      "               r3_pc     r5_pc    r10_pc    r15_pc    ...     High_p40  \\\n",
      "Date                                                  ...                \n",
      "2016-12-21  0.995450  1.000793  1.002070  0.988904    ...     1.139375   \n",
      "2010-09-23  1.009440  0.988193  0.961704  0.944080    ...     1.139112   \n",
      "2015-07-01  1.018215  1.039914  1.073794  1.089839    ...     0.830330   \n",
      "2013-12-12  0.993056  1.007176  1.010995  1.011130    ...     0.990518   \n",
      "2011-08-10  0.924576  0.983721  1.086769  1.140352    ...     0.800516   \n",
      "\n",
      "             Low_p40  Close_p40    r2_p40    r3_p40    r5_p40   r10_p40  \\\n",
      "Date                                                                      \n",
      "2016-12-21  1.126838   1.134862  1.136868  1.133692  1.139777  1.141231   \n",
      "2010-09-23  1.084293   1.137437  1.118166  1.109269  1.085921  1.056812   \n",
      "2015-07-01  0.811620   0.823075  0.813338  0.818238  0.835676  0.862902   \n",
      "2013-12-12  0.972695   0.981753  0.995778  1.002790  1.017049  1.020906   \n",
      "2011-08-10  0.731613   0.735226  0.778194  0.759226  0.807794  0.892413   \n",
      "\n",
      "             r15_p40   r25_p40   r40_p40  \n",
      "Date                                      \n",
      "2016-12-21  1.126236  1.084132  1.007409  \n",
      "2010-09-23  1.037446  1.001581  1.004677  \n",
      "2015-07-01  0.875796  0.892266  0.990712  \n",
      "2013-12-12  1.021042  1.017201  1.000424  \n",
      "2011-08-10  0.936413  0.969352  0.993361  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "Target found are: {1, -1}\n",
      "Sample of training data:\n",
      "Number of rows: 54228. Number of columns: 89.\n",
      "                   Vol   Open_pc   High_pc    Low_pc  Close_pc     r2_pc  \\\n",
      "Date                                                                       \n",
      "2012-05-30    256616.0  0.988334  0.996760  0.950097  0.971484  0.985742   \n",
      "2014-11-13   3686428.0  1.001573  1.020481  1.000000  1.015955  1.007977   \n",
      "2010-12-16  10421479.0  1.000000  1.006085  0.993915  1.005071  1.002535   \n",
      "2014-11-13   8477486.0  0.983190  0.983190  0.919383  0.963293  0.981647   \n",
      "2014-08-13  29265662.0  1.003982  1.019497  1.003021  1.012907  1.006453   \n",
      "\n",
      "               r3_pc     r5_pc    r10_pc    r15_pc    ...     High_p40  \\\n",
      "Date                                                  ...                \n",
      "2012-05-30  0.983366  0.992223  1.001685  1.023979    ...     0.775348   \n",
      "2014-11-13  1.005853  0.991923  0.979782  0.966888    ...     1.021688   \n",
      "2010-12-16  1.004733  1.003854  1.007201  0.992563    ...     0.966202   \n",
      "2014-11-13  0.990280  1.010086  1.041852  1.042653    ...     0.904408   \n",
      "2014-08-13  1.007094  1.005822  1.002266  1.010051    ...     1.075467   \n",
      "\n",
      "             Low_p40  Close_p40    r2_p40    r3_p40    r5_p40   r10_p40  \\\n",
      "Date                                                                      \n",
      "2012-05-30  0.739051   0.755687  0.766778  0.764930  0.771819  0.779180   \n",
      "2014-11-13  1.001183   1.017156  1.009169  1.007043  0.993096  0.980941   \n",
      "2010-12-16  0.954514   0.965228  0.962793  0.964904  0.964060  0.967274   \n",
      "2014-11-13  0.845713   0.886105  0.902988  0.910929  0.929148  0.958369   \n",
      "2014-08-13  1.058086   1.068515  1.061707  1.062383  1.061041  1.057289   \n",
      "\n",
      "             r15_p40   r25_p40   r40_p40  \n",
      "Date                                      \n",
      "2012-05-30  0.796522  0.896036  0.987750  \n",
      "2014-11-13  0.968032  0.977591  0.999315  \n",
      "2010-12-16  0.953216  0.965501  0.997760  \n",
      "2014-11-13  0.959105  0.945923  0.992466  \n",
      "2014-08-13  1.065502  1.032536  1.003393  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "Target found are: {1, -1}\n",
      "Sample of training data:\n",
      "Number of rows: 54228. Number of columns: 89.\n",
      "                   Vol   Open_pc   High_pc    Low_pc  Close_pc     r2_pc  \\\n",
      "Date                                                                       \n",
      "2016-12-21  61079086.0  1.000440  1.000440  0.989432  0.996477  0.998239   \n",
      "2010-09-23  14892838.0  0.992158  1.036597  0.986712  1.035072  1.017536   \n",
      "2015-07-01   4408307.0  1.012830  1.033262  1.009979  1.024234  1.012117   \n",
      "2013-12-12  30946095.0  0.978877  0.980903  0.963252  0.972222  0.986111   \n",
      "2011-08-10  99097088.0  0.974544  0.974859  0.890949  0.895349  0.947674   \n",
      "\n",
      "               r3_pc     r5_pc    r10_pc    r15_pc    ...     High_p40  \\\n",
      "Date                                                  ...                \n",
      "2016-12-21  0.995450  1.000793  1.002070  0.988904    ...     1.139375   \n",
      "2010-09-23  1.009440  0.988193  0.961704  0.944080    ...     1.139112   \n",
      "2015-07-01  1.018215  1.039914  1.073794  1.089839    ...     0.830330   \n",
      "2013-12-12  0.993056  1.007176  1.010995  1.011130    ...     0.990518   \n",
      "2011-08-10  0.924576  0.983721  1.086769  1.140352    ...     0.800516   \n",
      "\n",
      "             Low_p40  Close_p40    r2_p40    r3_p40    r5_p40   r10_p40  \\\n",
      "Date                                                                      \n",
      "2016-12-21  1.126838   1.134862  1.136868  1.133692  1.139777  1.141231   \n",
      "2010-09-23  1.084293   1.137437  1.118166  1.109269  1.085921  1.056812   \n",
      "2015-07-01  0.811620   0.823075  0.813338  0.818238  0.835676  0.862902   \n",
      "2013-12-12  0.972695   0.981753  0.995778  1.002790  1.017049  1.020906   \n",
      "2011-08-10  0.731613   0.735226  0.778194  0.759226  0.807794  0.892413   \n",
      "\n",
      "             r15_p40   r25_p40   r40_p40  \n",
      "Date                                      \n",
      "2016-12-21  1.126236  1.084132  1.007409  \n",
      "2010-09-23  1.037446  1.001581  1.004677  \n",
      "2015-07-01  0.875796  0.892266  0.990712  \n",
      "2013-12-12  1.021042  1.017201  1.000424  \n",
      "2011-08-10  0.936413  0.969352  0.993361  \n",
      "\n",
      "[5 rows x 89 columns]\n",
      "Epoch 1/10\n",
      "13700/54228 [======>.......................] - ETA: 5s - loss: 7.9037 - acc: 0.5096Epoch 1/10\n",
      "54228/54228 [==============================] - 7s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54228/54228 [==============================] - 7s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 2/10\n",
      "54228/54228 [==============================] - 6s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 3/10\n",
      "54228/54228 [==============================] - 7s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 3/10\n",
      "54228/54228 [==============================] - 7s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 4/10\n",
      "38300/54228 [====================>.........] - ETA: 2s - loss: 7.8852 - acc: 0.5108Epoch 1/10\n",
      "41250/54228 [=====================>........] - ETA: 1s - loss: 7.8832 - acc: 0.5109Epoch 1/10\n",
      "54228/54228 [==============================] - 7s - loss: 7.8783 - acc: 0.5112     \n",
      "10500/54228 [====>.........................] - ETA: 10s - loss: 8.0667 - acc: 0.4995Epoch 4/10\n",
      "54228/54228 [==============================] - 9s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 5/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      "Epoch 2/10\n",
      "11850/54228 [=====>........................] - ETA: 7s - loss: 7.8659 - acc: 0.5120\n",
      "Epoch 2/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "Epoch 5/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "45700/54228 [========================>.....] - ETA: 1s - loss: 8.1155 - acc: 0.4965Epoch 6/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      "Epoch 3/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      " 2150/54228 [>.............................] - ETA: 9s - loss: 7.8491 - acc: 0.5130Epoch 3/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "Epoch 6/10\n",
      "54228/54228 [==============================] - 9s - loss: 7.8783 - acc: 0.5112     \n",
      "Epoch 7/10\n",
      "52300/54228 [===========================>..] - ETA: 0s - loss: 8.1373 - acc: 0.4951\n",
      "Epoch 4/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      "Epoch 4/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "Epoch 7/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "Epoch 8/10\n",
      "44300/54228 [=======================>......] - ETA: 1s - loss: 7.8724 - acc: 0.5116\n",
      "Epoch 5/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      "11850/54228 [=====>........................] - ETA: 7s - loss: 7.9747 - acc: 0.5052Epoch 5/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "Epoch 8/10\n",
      "54228/54228 [==============================] - 9s - loss: 7.8783 - acc: 0.5112     \n",
      "44050/54228 [=======================>......] - ETA: 1s - loss: 8.1070 - acc: 0.4970Epoch 9/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      "Epoch 6/10\n",
      "54228/54228 [==============================] - 10s - loss: 8.1230 - acc: 0.4960    \n",
      "Epoch 6/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      " 9500/54228 [====>.........................] - ETA: 8s - loss: 7.9793 - acc: 0.5049Epoch 9/10\n",
      "44300/54228 [=======================>......] - ETA: 1s - loss: 8.0943 - acc: 0.4978\n",
      "Epoch 10/10\n",
      "45050/54228 [=======================>......] - ETA: 1s - loss: 7.8784 - acc: 0.5112\n",
      "Epoch 7/10\n",
      "54228/54228 [==============================] - 9s - loss: 8.1230 - acc: 0.4960     \n",
      "11150/54228 [=====>........................] - ETA: 7s - loss: 7.8350 - acc: 0.5139Epoch 7/10\n",
      "19250/54228 [=========>....................] - ETA: 6s - loss: 7.8539 - acc: 0.5127\n",
      "Epoch 10/10\n",
      "54228/54228 [==============================] - 10s - loss: 7.8783 - acc: 0.5112    \n",
      "54228/54228 [==============================] - 9s - loss: 8.1230 - acc: 0.4960     \n",
      "Epoch 8/10\n",
      "54228/54228 [==============================] - 9s - loss: 8.1230 - acc: 0.4960     \n",
      "Epoch 8/10\n",
      "54228/54228 [==============================] - 9s - loss: 7.8783 - acc: 0.5112     \n",
      "54228/54228 [==============================] - 8s - loss: 8.1230 - acc: 0.4960     \n",
      "Epoch 9/10\n",
      " 1400/54228 [..............................] - ETA: 8s - loss: 7.8058 - acc: 0.5157\n",
      "Epoch 9/10\n",
      "54228/54228 [==============================] - 8s - loss: 8.1230 - acc: 0.4960     \n",
      "Epoch 10/10\n",
      "54228/54228 [==============================] - 8s - loss: 8.1230 - acc: 0.4960     \n",
      "Epoch 10/10\n",
      "52450/54228 [============================>.] - ETA: 0s - loss: 8.1217 - acc: 0.4961\n",
      "54228/54228 [==============================] - 8s - loss: 8.1230 - acc: 0.4960     \n",
      "2688/2855 [===========================>..] - ETA: 0s\n",
      "acc: 51.14%\n",
      "2752/2855 [===========================>..] - ETA: 0s\n",
      "acc: 51.14%\n",
      "2272/2855 [======================>.......] - ETA: 0s\n",
      "acc: 49.60%\n",
      "2720/2855 [===========================>..] - ETA: 0s\n",
      "acc: 49.60%\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp/1521.577136.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/Gio/anaconda/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/Gio/anaconda/lib/python3.5/multiprocessing/pool.py\", line 47, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"<ipython-input-14-10ce8902bb38>\", line 141, in get_model_metrics\n    with open(\"temp/\"+str(time.clock())+\".json\", 'w') as fp:\nFileNotFoundError: [Errno 2] No such file or directory: 'temp/1521.577136.json'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp/1521.577136.json'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "\n",
    "filename = 'training_stocks.pkl'\n",
    "## Generate data to be inputted to the models\n",
    "X = get_training_stocks_df(filename)\n",
    "\n",
    "\n",
    "filename = 'testing_stocks.pkl'\n",
    "## Generate data of test stocks and save to csv files\n",
    "save_testing_stocks(filename)\n",
    "\n",
    "\n",
    "## Get performance of different types of models\n",
    "# for sell_below, buy_above in SELL_BUY_VALUES:\n",
    "#     for target_ratio in TARGET_RATIOS:\n",
    "#         get_model_metrics(target_ratio,sell_below,buy_above,X.copy())  ## copy to prevent updating\n",
    "#         print(\"Completed target ratio: {} with sell: {} and buy: {}\".format(target_ratio,sell_below,buy_above))\n",
    "\n",
    "    \n",
    "## Get the arguments for the pool\n",
    "args = []\n",
    "for sell_below, buy_above in SELL_BUY_VALUES:\n",
    "    for target_ratio in TARGET_RATIOS:\n",
    "        arg = (target_ratio,sell_below,buy_above,X.copy())\n",
    "        args.append(arg)\n",
    "\n",
    "## With multiprocessing\n",
    "pool = mp.Pool(processes=NUM_PROCESSES)\n",
    "pool.starmap(get_model_metrics,args)\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all temp json to append to result (used for multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9c93ca5372fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"temp/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp/'"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "directory = \"temp/\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"): \n",
    "        js = open(directory+filename).read()\n",
    "        temp_dict = json.loads(js)\n",
    "        result.append(temp_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column FinalValue per model.  Which takes taxes & commisions into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Go through each row\n",
    "for index, row in result_df.iterrows():\n",
    "    ## Initialize params\n",
    "    benchmark = {'value':0,'loss':0,'gain':0,'transactions':len(TESTING_TICKERS)*2.0}\n",
    "    decisiontree = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    gaussiannb = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    adaboost = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    svc = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    nn = {'value':0,'loss':0,'gain':0,'transactions':0}\n",
    "    \n",
    "    models_dict = {'benchmark':benchmark,'DecisionTree':decisiontree,'GaussianNB':gaussiannb,\\\n",
    "                   'Adaboost':adaboost, 'SVC':svc, 'NN':nn}\n",
    "    #models_dict = {'benchmark':benchmark,'NN':nn}\n",
    "    \n",
    "    ## Gather each stock information\n",
    "    for tick in TESTING_TICKERS:\n",
    "        ## update each models\n",
    "        for key, model in models_dict.items():\n",
    "            stock_value = row[tick+\"_\"+key]\n",
    "            if stock_value > MONEY:                         ## capital gain\n",
    "                model['gain'] += stock_value-MONEY\n",
    "            else:                                           ## loss\n",
    "                model['loss'] += MONEY-stock_value\n",
    "            \n",
    "            if key != 'benchmark':\n",
    "                model['transactions'] += row[tick+\"_\"+key+\"_transactions\"]\n",
    "        \n",
    "        \n",
    "    ## Get Final Values\n",
    "    for key, model in models_dict.items():\n",
    "        ## more gains than loss\n",
    "        if model['gain'] > model['loss']: \n",
    "            if key == 'benchmark':\n",
    "                model['value'] = float(format(row['total_'+key] - COMM_RATE*model['transactions'] \\\n",
    "                                                     - GAIN_LONG*(model['gain']-model['loss']),'.2f'))\n",
    "            else:\n",
    "                model['value'] = float(format(row['total_'+key] - COMM_RATE*model['transactions'] \\\n",
    "                                                     - GAIN_SHORT*(model['gain']-model['loss']),'.2f'))\n",
    "        ## more loss than gain\n",
    "        else:    \n",
    "            ## All model gains GAIN_LONG (assuming its on 25% tax bracket)\n",
    "            model['value'] = float(format(row['total_'+key] - COMM_RATE*model['transactions'] \\\n",
    "                                + GAIN_LONG*(model['loss']-model['gain']),'.2f')) ## add tax credit\n",
    "            \n",
    "\n",
    "        result_df.set_value(index,key+\"_FinalValue\",model['value'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df)\n",
    "result_df.to_csv('Results.csv')         ## can use parameters: mode='a', header=False, if memory is an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Execution Time: {}\".format(time.clock()-START))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
